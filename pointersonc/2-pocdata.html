<title>Data</title>

<link rel="stylesheet" href="../homepage/envoy.min.css">

<meta http-equiv="Content-Type" content="text/html; charset=utf-8">

<style>
	#code {
    width:900px;
    margin:0 auto;   
    /*temporary text styles below*/
    text-align:left;
    font-weight:bold;
    background: rgb(170,204,153);
  }
  main {
    margin-bottom: 200%;
  }
  .floating-menu {
    font-family: sans-serif;
    background: transparent;
    padding: 5px;;
    width: 80px;
    z-index: 100;
    position: fixed;
    top: 0px;
    right: 0px;
  }
  .floating-menu a, 
  .floating-menu h3 {
    font-size: 0.9em;
    display: block;
    margin: 0 0.5em;
    color: white;
  }
  .floating-menu a:hover{
    background: yellowgreen;
  }
</style>

  <nav class="floating-menu">
    <a href="../homepage2.html">Home</a>
    <a href="../bitcoin/0-bitcoinhomepage.html">Bitcoin</a>
    <a href="../crypto/0-cryptohomepage.html">Crypto</a>
    <a href="0-pochomepage.html">C</a>
    <a href="../linearalgebra/0-linearalgebrahomepage.html">Algebra</a>
  </nav>

<div class="col col-10 px2 js-linksHeight">
<h3 class="red h2 mt0 mb4 px3">
Data</h3>
<div class="mbn2 relative">
<div class="js-active-tab z2" style="pointer-events: none;"></div>
<ul class="list-style-none z3 js-galleryLinks">


<li class="p3 js-animateTab js-activeLink" data-tab="0">
<pre>
data: <font color=green>data types</font>; <font color=green>how to declare</font>; <font color=green>characterisics</font>;<font color=green>properties (scope, linkage, storage class)</font>
</pre>
</li>


<li class="p3 js-animateTab js-activeLink" data-tab="0">
<h3 class="mt0 mb1 slate blue h2">
Basic Data Types
</h3>
<pre>
Four basic data types = <font color=green>integers; floating-point values; pointers; aggregate types</font> such as arrays and structures.
</pre>
</li>


<li class="p3 js-animateTab js-activeLink" data-tab="0">
<h3 class="mt0 mb1 slate h5">
The Integer Family
</h3>
<pre>
integer family = <font color=blue>characters, short integers, integers, and long integers</font>.  All have both <font color=green>signed and unsigned</font> versions.

<font color=blue>Long integers are as least as large as integers, which themselves are at least as large as short integers.</font>

Type                      Minimum Range
___________________________________________________
char                      0 to 127
signed char               ‐127 to 127
unsigned char             0 to 255
___________________________________________________
short int                 ‐32767 to 32767
unsigned short int        0 to 65535
___________________________________________________
int                       ‐32767 to 32767
unsigned int              0 to 65535
___________________________________________________
long int                  ‐2147483647 to 2147483647
unsigned long int         0 to 4294967295
___________________________________________________

long long is added since <font color=green>C99</font>:
long long int             [−9,223,372,036,854,775,807, +9,223,372,036,854,775,807]
unsigned long long int    [0, +18,446,744,073,709,551,615]

Information about the actual properties, such as size, of the basic arithmetic types, is provided via macro constants in two headers:
<font color=green>&lt;limits.h&gt;</font> header (climits header in C++) defines macros for integer types and
<font color=green>&lt;float.h&gt;</font> header (cfloat header in C++) defines macros for floating-point types. The actual values depend on the implementation.

Properties of integer types; <a href="https://en.wikipedia.org/wiki/C_data_types">C Data Types</a>
CHAR_BIT – size of the char type in bits (at least 8 bits)
SCHAR_MIN, SHRT_MIN, INT_MIN, LONG_MIN, LLONG_MIN(C99) – minimum possible value of signed integer types: signed char, signed short, signed int, signed long, signed long long
SCHAR_MAX, SHRT_MAX, INT_MAX, LONG_MAX, LLONG_MAX(C99) – maximum possible value of signed integer types: signed char, signed short, signed int, signed long, signed long long
UCHAR_MAX, USHRT_MAX, UINT_MAX, ULONG_MAX, ULLONG_MAX(C99) – maximum possible value of unsigned integer types: unsigned char, unsigned short, unsigned int, unsigned long, unsigned long long
CHAR_MIN – minimum possible value of char
CHAR_MAX – maximum possible value of char
MB_LEN_MAX – maximum number of bytes in a multibyte character
</pre>

<pre id="code">
The code (%ld for signed long; %lu for unsigned long):
  printf("schar_min:%d\n",       SCHAR_MIN);
  printf("schar_max:%d\n",       SCHAR_MAX);
  printf("uchar_max:%d\n",       UCHAR_MAX);
  printf("shrt_min:%d\n",        SHRT_MIN);
  printf("shrt_max:%d\n",        SHRT_MAX);
  printf("int_min:%d\n",         INT_MIN);
  printf("int_max:%d\n",         INT_MAX);
  printf("uint_max:<font color=red>%ld</font>\n",       UINT_MAX);
  printf("long_min:%ld\n",       LONG_MIN);
  printf("long_max:%ld\n",       LONG_MAX);
  printf("ulong_max:<font color=red>%lu</font>\n",      ULONG_MAX);
  printf("longlong_min:%ld\n",   LLONG_MIN);
  printf("longlong_max:%ld\n",   LLONG_MAX);
  printf("char_bit:%ld\n",       CHAR_BIT);
  printf("char_min:%d\n",        CHAR_MIN);
  printf("char_max:%d\n",        CHAR_MAX);
  printf("mb_len_max:%ld\n",     MB_LEN_MAX);
 
Will print the following with -std= C89, C11 or C99:
  schar_min:     -128
  schar_max:      127
  uchar_max:      255
  shrt_min:      -32768
  shrt_max:       32767
  int_min:       -2147483648
  int_max:        2147483647
  uint_max:       4294967295
  long_min:      -9223372036854775808
  long_max:       9223372036854775807
  ulong_max:      18446744073709551615
  longlong_min:  -9223372036854775808
  longlong_max:   9223372036854775807
  char_bit:       8
  char_min:       -128
  char_max:       127
  mb_len_max:     6
</pre>

<pre>
Portability
The default char is always either a signed or an unsigned char, but what you get depends on the compiler.
So the portability of programs that use characters as little integers can be improved by explicitly declaring these variables as
either signed or unsigned. This practice ensures that their signed—ness is consistent from machine to machine.
</pre>
</li>


<li class="p3 js-animateTab js-activeLink" data-tab="0">
<h3 class="mt0 mb1 slate h5">
Integer Literals
</h3>
<pre>
The most natural way to specify decimal integer values, such as these:   123    65535    -275
Decimal literals are either int, long, or unsigned long. The shortest type that will contain the value is what is used by default.

but the default rules can be overridden for some literals by appending a suffix to the end of the value.
The characters L and l (that’s an el, not the digit one) cause a literal to be interpreted as a long integer value,
and the characters U and u specify an unsigned value. A literal may be designated unsigned long by appending ul to it.
e.g. int c = 100ul;

Integers can be given in octal by starting with the digit zero, or in hexadecimal by starting with 0x, as in
            0173            0177777           000060
            0x7b            0xFFFF            0xabcdef00

And then there are character literals. These always have type int; you cannot use the unsigned or long suffixes on them.
A character literal is a single character (or character escape or trigraph) enclosed in apostrophies, as in
            'M'         '\n'        \'??('            '\3777'

Multibye character literals such as ʹabcʹ are allowed by the Standard, but their implementation may vary from one environment
to the next so their use is discouraged.

Finally, wide character literals are written as an L followed by a multibyte character literal, as in:
L'X'     L'e^'
These are used when the runtime environment supports a large character set.

History of wide character:
<a href="https://en.wikipedia.org/wiki/Wide_character">https://en.wikipedia.org/wiki/Wide_character</a>
<a href="https://en.wikipedia.org/wiki/Universal_Coded_Character_Set">https://en.wikipedia.org/wiki/Universal_Coded_Character_Set</a>
<a href="https://en.wikipedia.org/wiki/Unicode">https://en.wikipedia.org/wiki/Unicode</a>
<a href="http://blog.csdn.net/zhangxinrun/article/details/5832260">http://blog.csdn.net/zhangxinrun/article/details/5832260</a>
<a href="http://www.cnblogs.com/iforever/p/4520692.html">http://www.cnblogs.com/iforever/p/4520692.html</a>

<b>History</b>
During the 1960s, mainframe and mini-computer manufacturers began to standardize around the 8-bit byte as their smallest datatype.
The 7-bit ASCII character set became the industry standard method for encoding alphanumeric characters for teletype machines and
computer terminals. The extra bit was used for parity, to ensure the integrity of data storage and transmission. As a result,
the 8-bit byte became the de facto datatype for computer systems storing ASCII characters in memory.

Later, computer manufacturers began to make use of the spare bit to extend the ASCII character set beyond its limited set of English
alphabet characters. 8-bit extensions such as IBM code page 37, PETSCII and ISO 8859 became commonplace, offering terminal support
for Greek, Cyrillic, and many others. However, such extensions were still limited in that they were region specific and often could
not be used in tandem. Special conversion routines had to be used to convert from one character set to another, often resulting in
destructive translation when no equivalent character existed in the target set.

In 1989, the International Organization for Standardization began work on the Universal Character Set (UCS), a multilingual character
set that could be encoded using either a 16-bit (2-byte) or 32-bit (4-byte) value. These larger values required the use of a datatype
larger than 8-bits to store the new character values in memory. Thus the term wide character was used to differentiate them from
traditional 8-bit character datatypes.

<b>Relation to UCS and Unicode</b>
A wide character refers to the size of the datatype in memory. It does not state how each value in a character set is defined.
Those values are instead defined using character sets, with UCS and Unicode simply being two common character sets that contain more
characters than an 8-bit value would allow.

<b>Relation to multibyte characters</b>
Just as earlier data transmission systems suffered from the lack of an 8-bit clean data path, modern transmission systems often lack
support for 16-bit or 32-bit data paths for character data. This has led to character encoding systems such as UTF-8 that can use
multiple bytes to encode a value that is too large for a single 8-bit symbol.

The C standard distinguishes between multibyte encodings of characters, which use a fixed or variable number of bytes to represent
each character (primarily used in source code and external files), from wide characters, which are run-time representations of
characters in single objects (typically, greater than 8 bits).

Size of a wide character
UTF-16 little-endian is the encoding standard at Microsoft (and in the Windows operating system). Yet with
surrogate pairs it supports 32-bit as well. The .Net Framework platform supports multiple wide-character
implementations including UTF7, UTF8, UTF16 and UTF32.

The Java platform requires that wide character variables be defined as 16-bit values, and that characters
be encoded using UTF-16 (due to former use of UCS-2), while modern Unix-like systems generally require
UTF-8 in their interfaces.

Programming specifics
C/C++
The C and C++ standard libraries include a number of facilities for dealing with wide characters and strings
composed of them. The wide characters are defined using datatype wchar_t, which in the original C90 standard was defined as

"an integral type whose range of values can represent distinct codes for all members of the largest extended
character set specified among the supported locales" (ISO 9899:1990 §4.1.5) Both C and C++ introduced fixed-size
character types char16_t and char32_t in the 2011 revisions of their respective standards to provide unambiguous
representation of 16-bit and 32-bit Unicode transformation formats, leaving wchar_t implementation-defined.
The ISO/IEC 10646:2003 Unicode standard 4.0 says that:

"The width of wchar_t is compiler-specific and can be as small as 8 bits. Consequently, programs that need to be
portable across any C or C++ compiler should not use wchar_t for storing Unicode text. The wchar_t type is intended
for storing compiler-defined wide characters, which may be Unicode characters in some compilers."

Python
According to Python's documentation, the language sometimes uses wchar_t as the basis for its character type Py_UNICODE.
It depends on whether wchar_t is "compatible with the chosen Python Unicode build variant" on that system.


如果一个数值是为了突出某几个位置上的bits，那最好还是把数字表示成八位或者十六位数值表示：
983040也许不代表什么特别的意义，但是写成0xF0000可能看得更清楚一些。

如果一个数字是为了表示某个字符，那么直接写出那个字符也许更好理解，因为字符也是个数字，以下是一样的：
value = value - 48;
value = value - \60;
value = value - '0';

</pre>
</li>


<li class="p3 js-animateTab js-activeLink" data-tab="0">
<h3 class="mt0 mb1 slate h5">
Enumerated Type
</h3>
<pre>
An enumerated type = one whose values are <font color=green>symbolic constants</font> rather than literal

e.g.: <font color=blue>enum Jar_Type {CUP, PINT, QUART, HALF_GALLON, GALLON};</font>
it declares a type called Jar_Type. variables of this type are declared like this:
<font color=blue>enum Jar_Type milk_jug, gas_can, medicine_bottle;</font>

They may also be combined like this, if there is only one declaration of variables of a particular enum type:
<font color=blue>enum { CUP, PINT, QUART, HALF_GALLON, GALLON }
      milk_jug, gas_can, medicine_bottle;</font>

这些变量是当做integer存储的，缺省情况下，CUP=0, PINT=1, ...依次类推，你也可以指定数值：
<font color=blue>enum Jar_Type { CUP=8, PINT=16, QUART=32, HALF_GALLON=64, GALLON=128 };</font>

如果有些symbol没有赋值，就用前面那个symbol的值加一，依次类推。

这些symbolic names (CUP, PINT...)被当做integer constants，整形常量。
</pre>
</li>

<li class="p3 js-animateTab " data-tab="1">
<h3 class="mt0 mb1 slate h5">
Floating-Point Types
</h3>
<pre>
floating-point family = float, double, long double
                        float: sinlge precision
                        double: double precision
                        long double: extended precisions if supported by the machine

<font color=red>Note:</font>
The term double precision is something of a misnomer because the precision is not really double.

The word double derives from the fact that a double-precision number uses twice as many bits as a regular
floating-point number.

For example, if a single-precision number requires 32 bits, its double-precision counterpart will be 64 bits long.

The extra bits increase not only the precision but also the range of magnitudes that can be represented.

The exact amount by which the precision and range of magnitudes are increased depends on what format the program
is using to represent floating-point values. Most computers use a standard format known as the IEEE floating-point format.

<font color=red>IEEE Single Precision</font>

The IEEE single precision floating point standard representation requires a 32 bit word, which may be represented as
numbered from 0 to 31, left to right.

The first bit is the sign bit, S,
the next eight bits are the exponent bits, 'E', and
the final 23 bits are the fraction 'F':

S EEEEEEEE FFFFFFFFFFFFFFFFFFFFFFF
0 1      8 9                    31
The value V represented by the word may be determined as follows:

If E=255 and F is nonzero, then V=NaN ("Not a number")
If E=255 and F is zero and S is 1, then V=-Infinity
If E=255 and F is zero and S is 0, then V=Infinity
If 0&lt;E&lt;255 then V=(-1)**S * 2 ** (E-127) * (1.F) where "1.F" is intended to represent the binary number created
by prefixing F with an implicit leading 1 and a binary point.

If E=0 and F is nonzero, then V=(-1)**S * 2 ** (-126) * (0.F). These are "unnormalized" values.
If E=0 and F is zero and S is 1, then V=-0
If E=0 and F is zero and S is 0, then V=0
In particular,

0 00000000 00000000000000000000000 = 0
1 00000000 00000000000000000000000 = -0

0 11111111 00000000000000000000000 = Infinity
1 11111111 00000000000000000000000 = -Infinity

0 11111111 00000100000000000000000 = NaN
1 11111111 00100010001001010101010 = NaN

0 10000000 00000000000000000000000 = +1 * 2**(128-127) * 1.0 = 2
0 10000001 10100000000000000000000 = +1 * 2**(129-127) * 1.101 = 6.5
1 10000001 10100000000000000000000 = -1 * 2**(129-127) * 1.101 = -6.5

0 00000001 00000000000000000000000 = +1 * 2**(1-127) * 1.0 = 2**(-126)
0 00000000 10000000000000000000000 = +1 * 2**(-126) * 0.1 = 2**(-127) 
0 00000000 00000000000000000000001 = +1 * 2**(-126) * 
                                     0.00000000000000000000001 = 
                                     2**(-149)  (Smallest positive value)
<font color=red>IEEE Double Precision</font>

The IEEE double precision floating point standard representation requires a 64 bit word, which may be represented as
numbered from 0 to 63, left to right.

The first bit is the sign bit, S,
the next eleven bits are the exponent bits, 'E', and
the final 52 bits are the fraction 'F':

S EEEEEEEEEEE FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
0 1        11 12                                                63
The value V represented by the word may be determined as follows:

If E=2047 and F is nonzero, then V=NaN ("Not a number")
If E=2047 and F is zero and S is 1, then V=-Infinity
If E=2047 and F is zero and S is 0, then V=Infinity
If 0&lt;E&lt;2047 then V=(-1)**S * 2 ** (E-1023) * (1.F) where "1.F" is intended to represent the binary number created
by prefixing F with an implicit leading 1 and a binary point.

If E=0 and F is nonzero, then V=(-1)**S * 2 ** (-1022) * (0.F) These are "unnormalized" values.
If E=0 and F is zero and S is 1, then V=-0
If E=0 and F is zero and S is 0, then V=0

Reference:
ANSI/IEEE Standard 754-1985


<font color=red>Floating-Point headers and literals</font>
&lt;float.h> header (cfloat header in C++) defines macros for floating-point types:
FLT_MIN, DBL_MIN, LDBL_MIN – minimum normalized positive value of float, double, long double respectively
FLT_TRUE_MIN, DBL_TRUE_MIN, LDBL_TRUE_MIN (C11) – minimum positive value of float, double, long double respectively
FLT_MAX, DBL_MAX, LDBL_MAX – maximum finite value of float, double, long double, respectively
FLT_ROUNDS – rounding mode for floating-point operations
FLT_EVAL_METHOD (C99) – evaluation method of expressions involving different floating-point types
FLT_RADIX – radix of the exponent in the floating-point types
FLT_DIG, DBL_DIG, LDBL_DIG – number of decimal digits that can be represented without losing precision by float,
double, long double, respectively

FLT_EPSILON, DBL_EPSILON, LDBL_EPSILON – difference between 1.0 and the next representable value of float, double,
long double, respectively

FLT_MANT_DIG, DBL_MANT_DIG, LDBL_MANT_DIG – number of FLT_RADIX-base digits in the floating-point significand for types
float, double, long double, respectively

FLT_MIN_EXP, DBL_MIN_EXP, LDBL_MIN_EXP – minimum negative integer such that FLT_RADIX raised to a power one less than
that number is a normalized float, double, long double, respectively

FLT_MIN_10_EXP, DBL_MIN_10_EXP, LDBL_MIN_10_EXP – minimum negative integer such that 10 raised to that power is a
normalized float, double, long double, respectively

FLT_MAX_EXP, DBL_MAX_EXP, LDBL_MAX_EXP – maximum positive integer such that FLT_RADIX raised to a power one less than
that number is a normalized float, double, long double, respectively

FLT_MAX_10_EXP, DBL_MAX_10_EXP, LDBL_MAX_10_EXP – maximum positive integer such that 10 raised to that power is a
normalized float, double, long double, respectively

DECIMAL_DIG (C99) – minimum number of decimal digits such that any number of the widest supported floating-point type
can be represented in decimal with a precision of DECIMAL_DIG digits and read back in the original floating-point type
without changing its value. DECIMAL_DIG is at least 10.

<font color=red>Floating-Point Literals:</font>
3.14        1E10         25.         .5         6.023e23
Floating point literals are double values unless they are followed by a suffix: L / l specify long double; 
                                                                                F / f specify float.
</pre>
</li>



<li class="p3 js-animateTab " data-tab="1">
<h3 class="mt0 mb1 slate h5">
Pointers
</h3>
<pre>
Pointer = Address
Pointer Variable = Variable whose value is and Address
</pre>
</li>



<li class="p3 js-animateTab " data-tab="1">
<h3 class="mt0 mb1 slate h5">
Pointer Constants
</h3>
<pre>
Usually not used except for one: NULL
</pre>
</li>



<li class="p3 js-animateTab " data-tab="1">
<h3 class="mt0 mb1 slate h5">
String Literals
</h3>
<pre>
C..strangely does not have string types, yet has string literals.
C string = a sequence of 0 or more characters terminated by NUL byte.

String literals causes the characters and NUL being stored somewhere in the program.
K&R C stated that string literals with same value are stored separately.
And it didn't mention whether characters in string literal could be modified by the program.
Still, many implementations supports modifications to string literals.

ANSI C states that the effect of modifying a string literal is undefined.  And it allows
the compiler to store a string literal once even if it appears multiple times in the program.
This makes modifying string literals very risky.

Better way to do it if you wish to modify a string literal is to put it in an array.
</pre>
</li>



<li class="p3 js-animateTab " data-tab="1">
<h3 class="mt0 mb1 slate blue h2">
Basic Declarations
</h3>
<pre>
<font color=green><i>spcifier(s) declaration_expression_list</i></font>
<font color=green><i>spcifier(s)</i></font>=base type + modifier to default storage class + scope of identifier
<font color=green><i>declaration_expression_list</i></font>=identifiers being declared + expression that shows how the identifiers are used

e.g. int i;
     char j, k, l;

specifiers may use keywords to modify the length / signedness of the identifier:
<font color=green>short / long / signed / unsigned</font>
also the type int may be omitted from the declaration of any integral type if at least one other specifier is given.

e.g. unsigned short int a;
     unsigned short     a;
     are the same declarations

     unsigned a;
     is also valid, should be an unsigned int
     
     equivalent declarations:
     short = signed short = short int = signed short int
     int   = signed int   = signed
     long  = signed long  = long int  = signed long int
     
     unsigned short = unsigned short int
     unsigned int   = unsigned
     unsigned long  = unsigned long int

<font color=red>signed</font> is usually used only for char because the other integer types are signed by default.
It is implementation dependent whether char is signed or unsigned.
</pre>
</li>


<li class="p3 js-animateTab " data-tab="1">
<h3 class="mt0 mb1 slate h5">
Initialization
</h3>
<pre>
a declaration may specify an initial value:
  int j = 15;

we return to initialization later..
</pre>
</li>



<li class="p3 js-animateTab " data-tab="1">
<h3 class="mt0 mb1 slate h5">
Declaring Simple Arrays
</h3>
<pre>
to declare a one-dimensional array = int values[20];
the name values when followed by a subscript, yields an value of type int

subscript of array always begin at zero, and ends at array_length - 1

note: compiler does not check the subscripts to see whether they are in proper range.
it is good because there is no time lost checking the subscripts
it is bad because invalid subscripts are not detected

initialization of array will be covered later...
</pre>
</li>




<li class="p3 js-animateTab " data-tab="1">
<h3 class="mt0 mb1 slate h5">
Declaring Pointers
</h3>
<pre>
in C, the base type is given first, followed by list of identifiers used in the expressions needed to produce the base type.
int *a;
states that the expression *a results in type int.
operator * performs indirection, we can infer this is a pointer to an integer.

int* b, c, d;   ->  int *b; int c; int d;
int *b, *c, *d; ->  int *b; int *c; int *d;

declaration while initialization:
char *message = "Hello World!";


</pre>
</li>




<li class="p3 js-animateTab " data-tab="1">
<h3 class="mt0 mb1 slate h5">
Implicit Declarations
</h3>
<pre>
there are a few declarations in which type names may be omitted.

functions, for example, are assumed to return integers unless declared otherwise.
any formal param of the function is assumed to be integers.
and if the compiler can determine the statement is a declaration, an omitted type is assumed to be integer.

e.g.
int a[10];
b[10];
d;
f( x )
{
    return x + 1;
}

ANSI C does not allow b[10]; and d;  It does allow function declaration without type:

<font color=green>$ gcc test.c -std=c11</font>
test.c:3:1: warning: return type defaults to 'int' [-Wimplicit-int]
 somefunc(x)
 ^~~~~~~~
test.c: In function 'somefunc':
test.c:3:1: warning: type of 'x' defaults to 'int' [-Wimplicit-int]

implicit declaration is not a good idea.  explict declaration may make your intent clear.
</pre>
</li>




<li class="p3 js-animateTab " data-tab="1">
<h3 class="mt0 mb1 slate blue h2">
Typedef
</h3>
<pre>
<font color=green>typedef</font> = define new names for various data types.
e.g. the declaration
     char *ptr_to_char;
declares the variable ptr_to_char to be a pointer to a character.  but add the keyword typedef you get:
     typedef char *ptr_to_char;
which declares the identifier ptr_to_char = new name for the type "pointer to char"
you can then use the new name in subsequent declarations:
     ptr_to_char a;
declares a to be a pointer to a character.

use typedef = reduce chance of mess up a declaration, particularly complex ones.

use typedef rather than #define for creating new type names, cause #define cannot handle pointer types.

e.g.   #define d_ptr_to_char char *
       d_ptr_to_char a, b;
       declares a properly, but declares b to be a char
</pre>
</li>



<li class="p3 js-animateTab " data-tab="1">
<h3 class="mt0 mb1 slate blue h2">
Constants
</h3>
<pre>
typedef
</pre>
</li>


<li class="p3 js-animateTab " data-tab="1">
<h3 class="mt0 mb1 slate h5">
Declaring Pointers
</h3>
<pre>
i
</pre>
</li>



<li class="p3 js-animateTab " data-tab="1">
<h3 class="mt0 mb1 slate blue h2">
Program Style
</h3>
<pre>

</pre>
</li>



</ul>
</div>

